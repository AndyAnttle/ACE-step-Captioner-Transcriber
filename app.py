import os
import time
import gc
import re
import threading
import numpy as np
import gradio as gr
import torch
import librosa
import soundfile as sf
import tempfile
from pathlib import Path
from transformers import (
    Qwen2_5OmniForConditionalGeneration,
    Qwen2_5OmniProcessor,
    TextStreamer,
    BitsAndBytesConfig,
    StoppingCriteria,
    StoppingCriteriaList,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    pipeline                      # –¥–ª—è Whisper pipeline
)
from qwen_omni_utils import process_mm_info
import tkinter as tk
from tkinter import filedialog
import noisereduce as nr
import torchaudio
from demucs import pretrained
from demucs.apply import apply_model

# ----------------------------------------------------------------------
# –ö—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è Qwen)
# ----------------------------------------------------------------------
class StopOnEvent(StoppingCriteria):
    def __init__(self, stop_event):
        self.stop_event = stop_event
    def __call__(self, input_ids, scores, **kwargs):
        return self.stop_event.is_set()

# ----------------------------------------------------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
# ----------------------------------------------------------------------
MODELS_ROOT = r"–≤–∞—à\–ø—É—Ç—å\–ø–∞–ø–∫–∞"
SAMPLE_RATE = 16000
DEFAULT_SYSTEM_PROMPT = (
    "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, "
    "capable of perceiving auditory and visual inputs, as well as generating text and speech."
)

current_model = None           # –¥–ª—è Qwen
current_processor = None       # –¥–ª—è Qwen
whisper_model = None           # –¥–ª—è Whisper
whisper_processor = None       # –¥–ª—è Whisper
whisper_pipeline = None        # –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è Whisper
current_model_path = None
current_model_type = None
current_stop_event = None
batch_stop_flag = False

# ----------------------------------------------------------------------
# –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π Qwen (–ø—Ä–æ—Å—Ç–æ–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
# ----------------------------------------------------------------------
def find_models_simple(root_dir):
    models = []
    if not os.path.exists(root_dir):
        return models
    for item in os.listdir(root_dir):
        item_path = os.path.join(root_dir, item)
        if os.path.isdir(item_path):
            lower = item.lower()
            if "captioner" in lower:
                models.append({"name": item, "path": item_path, "type": "captioner"})
            elif "transcriber" in lower:
                models.append({"name": item, "path": item_path, "type": "transcriber"})
    return models

def find_models_advanced(root_dir):
    models = []
    if not os.path.exists(root_dir):
        return models
    for item in os.listdir(root_dir):
        item_path = os.path.join(root_dir, item)
        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, "config.json")):
            lower = item.lower()
            if "captioner" in lower:
                model_type = "captioner"
            elif "transcriber" in lower:
                model_type = "transcriber"
            else:
                model_type = "captioner"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            models.append({"name": item, "path": item_path, "type": model_type})
    return models

def get_models(advanced):
    if advanced:
        return find_models_advanced(MODELS_ROOT)
    else:
        return find_models_simple(MODELS_ROOT)

# ----------------------------------------------------------------------
# –í—ã–≥—Ä—É–∑–∫–∞ Qwen
# ----------------------------------------------------------------------
def unload_qwen_model():
    global current_model, current_processor, current_model_path, current_model_type
    if current_model is not None:
        print("\n=== –í–´–ì–†–£–ó–ö–ê QWEN ===")
        del current_model
        del current_processor
        current_model = None
        current_processor = None
        current_model_path = None
        current_model_type = None
        gc.collect()
        torch.cuda.empty_cache()
        print("Qwen –≤—ã–≥—Ä—É–∂–µ–Ω.")

# ----------------------------------------------------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ Qwen
# ----------------------------------------------------------------------
def load_qwen_model(model_info, dtype_choice, quantization_choice, use_flash, disable_talker,
                    device_map_strategy, custom_limits_str, offload_folder):
    global current_model, current_processor, current_model_path, current_model_type

    unload_qwen_model()

    path = model_info["path"]
    model_type = model_info["type"]

    if quantization_choice != "None":
        try:
            import bitsandbytes
        except ImportError:
            return (gr.update(), "‚ùå bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", gr.update())

    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
        "float8_e4m3fn": torch.float8_e4m3fn if hasattr(torch, 'float8_e4m3fn') else None,
        "float8_e5m2": torch.float8_e5m2 if hasattr(torch, 'float8_e5m2') else None
    }
    torch_dtype = dtype_map.get(dtype_choice, torch.float16)
    if torch_dtype is None:
        return (gr.update(), f"‚ùå –¢–∏–ø {dtype_choice} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —ç—Ç–æ–π –≤–µ—Ä—Å–∏–µ–π torch", gr.update())

    quantization_config = None
    if quantization_choice == "8-bit":
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        torch_dtype = torch.float16
    elif quantization_choice == "4-bit":
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        torch_dtype = torch.float16

    device_map = device_map_strategy if device_map_strategy != "custom" else "auto"
    max_memory = None
    if device_map_strategy == "custom":
        max_memory, err = parse_memory_limits(custom_limits_str)
        if err:
            return (gr.update(), f"‚ùå –û—à–∏–±–∫–∞ –≤ –ª–∏–º–∏—Ç–∞—Ö: {err}", gr.update())

    attn_impl = "flash_attention_2" if use_flash else None

    print(f"\n=== –ó–ê–ì–†–£–ó–ö–ê QWEN: {model_info['name']} ===")
    start_load = time.time()
    try:
        processor = Qwen2_5OmniProcessor.from_pretrained(path, trust_remote_code=True)
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            path,
            torch_dtype=torch_dtype,
            quantization_config=quantization_config,
            device_map=device_map,
            max_memory=max_memory,
            trust_remote_code=True,
            attn_implementation=attn_impl,
            offload_folder=offload_folder if offload_folder else None
        )
        if disable_talker and hasattr(model, "disable_talker"):
            model.disable_talker()
            print("Talker –æ—Ç–∫–ª—é—á—ë–Ω.")
        model.eval()
    except Exception as e:
        print(f"–û–®–ò–ë–ö–ê: {e}")
        return (gr.update(), f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}", gr.update())

    current_model = model
    current_processor = processor
    current_model_path = path
    current_model_type = model_type

    load_time = time.time() - start_load
    print(f"Qwen –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {model.device}, dtype: {model.dtype}")

    if model_type == "captioner":
        default_prompt = "*Task* Describe this audio in detail. Provide only textual description."
        btn_text = "üéµ –û–ø–∏—Å–∞—Ç—å –º—É–∑—ã–∫—É"
    else:
        default_prompt = "*Task* Transcribe this audio in detail. Provide structured lyrics with section tags."
        btn_text = "üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å"

    btn_update = gr.update(value=btn_text, variant="primary", interactive=True)
    status_msg = f"‚úÖ –ú–æ–¥–µ–ª—å {model_info['name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–≤—Ä–µ–º—è: {load_time:.1f}—Å)"
    prompt_update = gr.update(value=default_prompt)

    return (btn_update, status_msg, prompt_update)

# ----------------------------------------------------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ Whisper (–º–æ–¥–µ–ª—å –∏ –ø–∞–π–ø–ª–∞–π–Ω)
# ----------------------------------------------------------------------
def load_whisper_model(model_version="openai/whisper-large-v3"):
    global whisper_model, whisper_processor, whisper_pipeline
    # –ï—Å–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ç–∞ –∂–µ –≤–µ—Ä—Å–∏—è, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
    if whisper_pipeline is not None and hasattr(whisper_model, "config") and whisper_model.config._name_or_path == model_version:
        return
    # –í—ã–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞
    unload_whisper_model()
    device = 0 if torch.cuda.is_available() else -1
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ Whisper {model_version}...")
    whisper_processor = WhisperProcessor.from_pretrained(model_version)
    whisper_model = WhisperForConditionalGeneration.from_pretrained(model_version).to(device if device >= 0 else "cpu")
    whisper_model.eval()
    whisper_pipeline = pipeline(
        "automatic-speech-recognition",
        model=whisper_model,
        tokenizer=whisper_processor.tokenizer,
        feature_extractor=whisper_processor.feature_extractor,
        chunk_length_s=30,
        batch_size=16,
        device=device,
    )
    print(f"‚úÖ Whisper {model_version} –∑–∞–≥—Ä—É–∂–µ–Ω")

# ----------------------------------------------------------------------
# –í—ã–≥—Ä—É–∑–∫–∞ Whisper
# ----------------------------------------------------------------------
def unload_whisper_model():
    global whisper_model, whisper_processor, whisper_pipeline
    if whisper_pipeline is not None:
        print("\n=== –í–´–ì–†–£–ó–ö–ê WHISPER ===")
        del whisper_pipeline
        del whisper_model
        del whisper_processor
        whisper_pipeline = None
        whisper_model = None
        whisper_processor = None
        gc.collect()
        torch.cuda.empty_cache()
        print("Whisper –≤—ã–≥—Ä—É–∂–µ–Ω.")

# ----------------------------------------------------------------------
# –ü–∞—Ä—Å–∏–Ω–≥ –ª–∏–º–∏—Ç–æ–≤ –ø–∞–º—è—Ç–∏
# ----------------------------------------------------------------------
def parse_memory_limits(limit_str):
    if not limit_str or limit_str.strip() == "":
        return None, None
    parts = [p.strip() for p in limit_str.split(",")]
    max_memory = {}
    pattern = re.compile(r"^(\d+(?:\.\d+)?)\s*(GB|MB|GiB|MiB)?$", re.IGNORECASE)
    for i, part in enumerate(parts):
        if not part:
            continue
        m = pattern.match(part)
        if not m:
            return None, f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è GPU {i}: '{part}'"
        num = float(m.group(1))
        unit = (m.group(2) or "GB").upper()
        if unit in ("GB", "GIB"):
            pass
        elif unit in ("MB", "MIB"):
            num = num / 1024.0
        else:
            return None, f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ '{unit}'"
        max_memory[i] = f"{num:.0f}GB"
    return max_memory, None

# ----------------------------------------------------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
# ----------------------------------------------------------------------
def ensure_output_dir(output_dir):
    if output_dir:
        Path(output_dir).mkdir(parents=True, exist_ok=True)

def save_result(text, audio_path, output_dir, model_type):
    if not text or text.startswith("‚ö†") or text.startswith("–û—à–∏–±–∫–∞") or text.startswith("‚èπÔ∏è"):
        return None, "–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω (–ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—à–∏–±–æ—á–Ω—ã–π)"
    if audio_path and isinstance(audio_path, str) and os.path.exists(audio_path):
        base = os.path.splitext(os.path.basename(audio_path))[0]
    else:
        base = "result"
    suffix = "caption" if model_type == "captioner" else "transcript"
    filename = f"{base}_{suffix}.txt"
    if output_dir:
        ensure_output_dir(output_dir)
        save_path = os.path.join(output_dir, filename)
    elif audio_path and isinstance(audio_path, str):
        save_path = os.path.join(os.path.dirname(audio_path), filename)
    else:
        save_path = filename
    try:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(text)
        return save_path, f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}"
    except Exception as e:
        return None, f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}"

def save_debug_audio(audio_data, sr, prefix="qwen_input"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—É–¥–∏–æ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    timestamp = int(time.time())
    filename = f"{prefix}_{timestamp}.wav"
    temp_dir = tempfile.gettempdir()
    path = os.path.join(temp_dir, filename)
    sf.write(path, audio_data, sr)
    print(f"üíæ DEBUG: –∞—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {path}")
    return path

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ –ø–∞–ø–∫–∏ —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥
# ----------------------------------------------------------------------
def browse_folder(current_path=""):
    root = tk.Tk()
    root.withdraw()
    root.attributes('-topmost', True)
    initial_dir = current_path if current_path and os.path.exists(current_path) else "."
    selected = filedialog.askdirectory(title="–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞–ø–∫—É", initialdir=initial_dir)
    root.destroy()
    return selected if selected else current_path

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Å—Ç–µ–º—ã (Demucs)
# ----------------------------------------------------------------------
def separate_stems(audio_path, target_stem='vocals', use_gpu=False):
    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
    print(f"üé§ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Demucs (htdemucs) –Ω–∞ {device}...")
    model = pretrained.get_model('htdemucs')
    model.to(device)
    model.eval()

    wav, orig_sr = torchaudio.load(audio_path)
    target_sr = 44100
    if orig_sr != target_sr:
        print(f"üîÅ –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ —Å {orig_sr} –ì—Ü –¥–æ {target_sr} –ì—Ü...")
        resampler = torchaudio.transforms.Resample(orig_sr, target_sr)
        wav = resampler(wav)
        sr = target_sr
    else:
        sr = orig_sr

    if wav.shape[0] == 1:
        wav = wav.repeat(2, 1)
    wav = wav.unsqueeze(0).to(device)

    print("üîÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ç–µ–º—ã...")
    with torch.no_grad():
        sources = apply_model(model, wav, shifts=5, split=True, overlap=0.25, progress=True)[0]

    stems = model.sources  # ['drums', 'bass', 'other', 'vocals']

    if target_stem == 'instrumental':
        vocal_idx = stems.index('vocals')
        instrumental = torch.cat([sources[i] for i in range(len(stems)) if i != vocal_idx], dim=0).mean(dim=0, keepdim=True)
        result_audio = instrumental.cpu().numpy().squeeze()
    else:
        stem_idx = stems.index(target_stem)
        result_audio = sources[stem_idx].cpu().numpy().squeeze()
        if result_audio.ndim > 1:
            result_audio = result_audio.mean(axis=0)

    # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –æ–±—Ä–∞—Ç–Ω–æ –¥–æ 16 –∫–ì—Ü –¥–ª—è –º–æ–¥–µ–ª–µ–π
    if sr != SAMPLE_RATE:
        print(f"üîÅ –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å {sr} –ì—Ü –¥–æ {SAMPLE_RATE} –ì—Ü...")
        result_audio = librosa.resample(result_audio, orig_sr=sr, target_sr=SAMPLE_RATE)

    return result_audio, SAMPLE_RATE

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∞—É–¥–∏–æ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã
# ----------------------------------------------------------------------
def split_audio(audio_data, sr, segment_sec, overlap_sec=0):
    samples_per_seg = int(segment_sec * sr)
    total_samples = len(audio_data)
    segments = []
    start = 0
    while start < total_samples:
        end = min(start + samples_per_seg, total_samples)
        segment = audio_data[start:end]
        segments.append(segment)
        start += samples_per_seg
    return segments

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ Whisper (–±–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è pipeline)
# ----------------------------------------------------------------------
def transcribe_with_whisper_pipeline(audio_data, sr, language=None):
    global whisper_pipeline
    if whisper_pipeline is None:
        load_whisper_model()
    if sr != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)
        sr = 16000
    generate_kwargs = {
        "task": "transcribe",
        "temperature": 0.0,  # –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
    }
    if language is not None and language != "auto":
        generate_kwargs["language"] = language
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã no_speech_threshold, logprob_threshold –∏ –¥—Ä. –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å,
    # –Ω–æ –æ–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ.
    result = whisper_pipeline(audio_data, generate_kwargs=generate_kwargs)
    return result["text"]

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ Whisper —Å —Ä—É—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
# ----------------------------------------------------------------------
def transcribe_with_whisper_manual(audio_data, sr, language=None):
    global whisper_model, whisper_processor
    if whisper_model is None:
        load_whisper_model()
    if sr != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)
        sr = 16000
    # –ü–æ–ª—É—á–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    inputs = whisper_processor(audio_data, sampling_rate=sr, return_tensors="pt")
    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
    inputs = {k: v.to(whisper_model.device) for k, v in inputs.items()}
    # –ü–æ–ª—É—á–∞–µ–º forced_decoder_ids –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è —è–∑—ã–∫–∞ –∏ –∑–∞–¥–∞—á–∏
    forced_decoder_ids = whisper_processor.get_decoder_prompt_ids(language=language, task="transcribe")
    with torch.no_grad():
        predicted_ids = whisper_model.generate(
            **inputs,
            forced_decoder_ids=forced_decoder_ids,
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (temperature, no_speech_threshold –∏ —Ç.–¥.) –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ generate
            # –î–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º do_sample=False (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        )
    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

# ----------------------------------------------------------------------
# –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Qwen (–æ—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
# ----------------------------------------------------------------------
def process_with_qwen(audio_data, sr, text_input, system_prompt, user_prompt,
                      max_new_tokens, do_sample, temperature, repetition_penalty,
                      console_progress):
    global current_model, current_processor, current_model_type, current_stop_event

    if current_model is None:
        return "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Qwen."

    # –§–æ—Ä–º–∏—Ä—É–µ–º conversation
    conversations = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]}
    ]
    user_content = [{"type": "audio", "audio": audio_data}, {"type": "text", "text": user_prompt}]
    conversations.append({"role": "user", "content": user_content})

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    try:
        audios_list = [audio_data]
        inputs = current_processor.apply_chat_template(
            conversations,
            audios=audios_list,
            load_audio_from_video=False,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
            padding=True,
        ).to(current_model.device)
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}"

    for k, v in inputs.items():
        if isinstance(v, torch.Tensor) and torch.is_floating_point(v) and v.dtype != current_model.dtype:
            inputs[k] = v.to(current_model.dtype)

    gen_kwargs = {
        "max_new_tokens": max_new_tokens,
        "pad_token_id": current_processor.tokenizer.pad_token_id or current_processor.tokenizer.eos_token_id,
        "do_sample": do_sample,
        "return_audio": False,
        "repetition_penalty": repetition_penalty,
    }
    if do_sample:
        gen_kwargs["temperature"] = temperature

    if console_progress:
        streamer = TextStreamer(current_processor.tokenizer, skip_prompt=True)
        gen_kwargs["streamer"] = streamer

    stop_event = threading.Event()
    current_stop_event = stop_event
    stopping_criteria = StoppingCriteriaList([StopOnEvent(stop_event)])
    gen_kwargs["stopping_criteria"] = stopping_criteria

    try:
        with torch.no_grad():
            output = current_model.generate(**inputs, **gen_kwargs)
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
    finally:
        current_stop_event = None

    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if isinstance(output, torch.Tensor):
        text_ids = output
    elif isinstance(output, tuple):
        text_ids = output[0]
    else:
        text_ids = output

    input_len = inputs["input_ids"].shape[1]
    response_ids = text_ids[0][input_len:]
    response = current_processor.decode(response_ids, skip_special_tokens=True)

    if stop_event.is_set():
        response = "‚ö†Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (—á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç).\n\n" + response

    return response

# ----------------------------------------------------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∂–∏–º–∞)
# ----------------------------------------------------------------------
def process_single_file(audio_file, text_input, system_prompt, user_prompt,
                        max_sec, max_new_tokens, do_sample, temperature, repetition_penalty,
                        console_progress, noise_reduction, use_stem_separation, target_stem,
                        use_gpu_demucs, save_debug, recognition_model, use_segmentation,
                        segment_duration, whisper_language):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∞—É–¥–∏–æ—Ñ–∞–π–ª:
    - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å—Ç–µ–º—ã, —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ, –æ–±—Ä–µ–∑–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    - –ó–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞—á–∞ –≤ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å —Å —É—á—ë—Ç–æ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    """
    global batch_stop_flag

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ (–µ–¥–∏–Ω–∞—è —á–∞—Å—Ç—å)
    if isinstance(audio_file, tuple) and len(audio_file) == 2:
        sr, arr = audio_file
        tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        sf.write(tmp.name, arr, sr)
        audio_file = tmp.name

    if not os.path.exists(audio_file):
        return "–û—à–∏–±–∫–∞: –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω."

    try:
        if use_stem_separation:
            print(f"üé§ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ç–µ–º—ã (target: {target_stem})...")
            try:
                audio_data, sr = separate_stems(audio_file, target_stem, use_gpu=use_gpu_demucs)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∞—É–¥–∏–æ.")
                audio_data, sr = librosa.load(audio_file, sr=SAMPLE_RATE, mono=True)
        else:
            audio_data, sr = librosa.load(audio_file, sr=SAMPLE_RATE, mono=True)

        print(f"    —á–∞—Å—Ç–æ—Ç–∞: {sr} –ì—Ü, —Ñ–æ—Ä–º–∞: {audio_data.shape}")
        dur = len(audio_data) / sr
        print(f"    –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {dur:.2f} —Å–µ–∫")

        if noise_reduction:
            print("    –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ...")
            audio_data = nr.reduce_noise(y=audio_data, sr=sr, prop_decrease=0.8)

        if max_sec > 0:
            max_samp = int(max_sec * sr)
            if len(audio_data) > max_samp:
                audio_data = audio_data[:max_samp]
                print(f"    –æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ {max_sec} —Å–µ–∫")

        if np.max(np.abs(audio_data)) > 1.0:
            audio_data = audio_data / np.max(np.abs(audio_data))
            print("    –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è")

        if save_debug:
            save_func = globals()['save_debug_audio']
            save_func(audio_data, sr, prefix="processed_audio")

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ: {e}"

    # –¢–µ–ø–µ—Ä—å –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∂–∏–º–∞
    if recognition_model == "Whisper Large v3":
        if use_segmentation:
            # –†—É—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è Whisper
            segments = split_audio(audio_data, sr, segment_duration)
            print(f"–†—É—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            full_text = ""
            for idx, seg in enumerate(segments):
                if batch_stop_flag:
                    full_text += "\n‚èπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞"
                    break
                print(f"--- –°–µ–≥–º–µ–Ω—Ç {idx+1}/{len(segments)} ---")
                seg_text = transcribe_with_whisper_manual(seg, sr, language=whisper_language if whisper_language != "auto" else None)
                full_text += seg_text + " "
            return full_text.strip()
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ pipeline
            return transcribe_with_whisper_pipeline(audio_data, sr, language=whisper_language if whisper_language != "auto" else None)
    else:
        # Qwen Omni (—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∂–µ)
        return process_with_qwen(audio_data, sr, text_input, system_prompt, user_prompt,
                                 max_new_tokens, do_sample, temperature, repetition_penalty,
                                 console_progress)

# ----------------------------------------------------------------------
# –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ–±—ë—Ä—Ç–∫–∞)
# ----------------------------------------------------------------------
def batch_process(file_list, folder_path, text_input, system_prompt, user_prompt,
                  max_audio_sec, max_new_tokens, do_sample, temperature,
                  repetition_penalty, auto_save, output_dir, console_progress,
                  noise_reduction, use_segmentation, segment_duration,
                  use_stem_separation, target_stem, use_gpu_demucs, save_debug,
                  recognition_model, whisper_language, progress=gr.Progress()):
    global batch_stop_flag, current_model_type

    if recognition_model == "Qwen Omni" and current_model is None:
        return "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Qwen."
    if recognition_model == "Whisper Large v3":
        load_whisper_model()  # –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è, –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

    audio_files = []
    if file_list:
        audio_files.extend(file_list)
    if folder_path and os.path.isdir(folder_path):
        for f in os.listdir(folder_path):
            if f.lower().endswith(('.wav', '.mp3', '.flac', '.m4a', '.ogg')):
                audio_files.append(os.path.join(folder_path, f))

    if not audio_files:
        return "–ù–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."

    if auto_save and output_dir:
        ensure_output_dir(output_dir)

    results = []
    total = len(audio_files)
    for i, audio_path in enumerate(audio_files):
        if batch_stop_flag:
            print("–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            results.append("‚èπÔ∏è –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break

        progress(i/total, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {os.path.basename(audio_path)}")
        print(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i+1}/{total}: {audio_path} ---")
        try:
            response = process_single_file(
                audio_path, text_input, system_prompt, user_prompt,
                max_audio_sec, max_new_tokens, do_sample, temperature, repetition_penalty,
                console_progress, noise_reduction, use_stem_separation, target_stem,
                use_gpu_demucs, save_debug, recognition_model, use_segmentation,
                segment_duration, whisper_language
            )
        except Exception as e:
            response = f"‚èπÔ∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞): {e}"

        if batch_stop_flag:
            print("–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø–æ—Å–ª–µ —Ñ–∞–π–ª–∞.")
            results.append("‚èπÔ∏è –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞.")
            break

        if auto_save and not response.startswith("‚ö†") and not response.startswith("–û—à–∏–±–∫–∞") and not response.startswith("‚èπÔ∏è"):
            model_type_for_save = current_model_type if recognition_model == "Qwen Omni" else "whisper"
            saved_path, save_msg = save_result(response, audio_path, output_dir, model_type_for_save)
            if saved_path:
                results.append(f"{os.path.basename(audio_path)} -> {saved_path}")
            else:
                results.append(f"{os.path.basename(audio_path)}: {save_msg}")
        else:
            results.append(f"{os.path.basename(audio_path)}:\n{response}\n")

    summary = "\n".join(results)
    return summary

# ----------------------------------------------------------------------
# –§—É–Ω–∫—Ü–∏–∏ —Å–±—Ä–æ—Å–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
# ----------------------------------------------------------------------
def reset_system_prompt():
    return DEFAULT_SYSTEM_PROMPT

def reset_user_prompt():
    if current_model_type == "captioner":
        return "*Task* Describe this audio in detail. Provide only textual description."
    elif current_model_type == "transcriber":
        return "*Task* Transcribe this audio in detail. Provide structured lyrics with section tags."
    else:
        return ""

# ----------------------------------------------------------------------
# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# ----------------------------------------------------------------------
def stop_generation():
    global current_stop_event, batch_stop_flag
    print(">>> stop_generation –≤—ã–∑–≤–∞–Ω–∞, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º batch_stop_flag = True")
    batch_stop_flag = True
    if current_stop_event is not None:
        current_stop_event.set()
        return "‚èπÔ∏è –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è –Ω–∞ –±–ª–∏–∂–∞–π—à–µ–º —à–∞–≥–µ)"
    return "‚èπÔ∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–æ –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—É–¥–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"

# ----------------------------------------------------------------------
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
# ----------------------------------------------------------------------
def prepare_download(text, audio_path, output_dir, model_type):
    if not text or text.startswith("‚ö†") or text.startswith("–û—à–∏–±–∫–∞") or text.startswith("‚èπÔ∏è"):
        return None, "–ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"
    if audio_path and isinstance(audio_path, str) and os.path.exists(audio_path):
        base = os.path.splitext(os.path.basename(audio_path))[0]
    else:
        base = "result"
    suffix = "caption" if model_type == "captioner" else "transcript"
    filename = f"{base}_{suffix}.txt"
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, filename)
    try:
        with open(temp_path, 'w', encoding='utf-8') as f:
            f.write(text)
        return temp_path, f"‚úÖ –§–∞–π–ª –≥–æ—Ç–æ–≤ –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é: {filename}"
    except Exception as e:
        return None, f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"

# ----------------------------------------------------------------------
# –ò–ù–¢–ï–†–§–ï–ô–° GRADIO
# ----------------------------------------------------------------------
initial_models = get_models(False)
model_choices = [(m["name"], m) for m in initial_models]

# –°–ø–∏—Å–æ–∫ —è–∑—ã–∫–æ–≤ –¥–ª—è Whisper
whisper_languages = ["auto", "ru", "en", "de", "fr", "es", "it", "pt", "pl", "uk", "zh", "ja", "ko"]
# –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ Whisper
whisper_versions = ["openai/whisper-large-v3", "openai/whisper-large-v2"]

with gr.Blocks(title="üéµ ACE-step: Captioner & Transcriber") as demo:
    gr.Markdown("# üéµ ACE-step: Captioner & Transcriber")
    gr.Markdown("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å, –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ.")

    # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞: –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –Ω–µ—ë —ç–ª–µ–º–µ–Ω—Ç—ã
    with gr.Row():
        recognition_model = gr.Radio(choices=["Qwen Omni", "Whisper Large"], value="Qwen Omni", label="–ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è", scale=2)

        # –≠–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è Whisper (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ —Å–∫—Ä—ã—Ç—ã)
        with gr.Column(scale=3, visible=False) as whisper_col:
            whisper_version = gr.Dropdown(choices=whisper_versions, value="openai/whisper-large-v3", label="–í–µ—Ä—Å–∏—è Whisper")
            whisper_language = gr.Dropdown(choices=whisper_languages, value="auto", label="–Ø–∑—ã–∫ (Whisper)")

        # –≠–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è Qwen (–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –≤–∏–¥–Ω—ã)
        with gr.Column(scale=3, visible=True) as qwen_col:
            model_dropdown = gr.Dropdown(choices=model_choices, label="–ú–æ–¥–µ–ª—å Qwen")
            advanced_search = gr.Checkbox(label="üîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫", value=False)

        # –ö–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (–≤—Å–µ–≥–¥–∞ –≤–∏–¥–Ω—ã, –Ω–æ –∞–∫—Ç–∏–≤–Ω—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±–æ—Ä–∞)
        load_qwen_btn = gr.Button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å Qwen", variant="primary", scale=1, visible=True)
        unload_qwen_btn = gr.Button("‚èπÔ∏è –í—ã–≥—Ä—É–∑–∏—Ç—å Qwen", variant="secondary", scale=1, visible=True)
        load_whisper_btn = gr.Button("üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å Whisper", variant="primary", scale=1, visible=False)
        unload_whisper_btn = gr.Button("‚èπÔ∏è –í—ã–≥—Ä—É–∑–∏—Ç—å Whisper", variant="secondary", scale=1, visible=False)

    # –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≤–∏–¥–∏–º–æ—Å—Ç–∏ –±–ª–æ–∫–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    def toggle_ui(choice):
        show_qwen = choice == "Qwen Omni"
        show_whisper = choice == "Whisper Large v3"
        return (
            gr.update(visible=show_whisper),        # whisper_col
            gr.update(visible=show_qwen),            # qwen_col
            gr.update(visible=show_qwen),            # load_qwen_btn
            gr.update(visible=show_qwen),            # unload_qwen_btn
            gr.update(visible=show_whisper),         # load_whisper_btn
            gr.update(visible=show_whisper),         # unload_whisper_btn
        )

    recognition_model.change(
        fn=toggle_ui,
        inputs=recognition_model,
        outputs=[whisper_col, qwen_col, load_qwen_btn, unload_qwen_btn, load_whisper_btn, unload_whisper_btn]
    )

    def update_model_list(advanced):
        models = get_models(advanced)
        return gr.Dropdown(choices=[(m["name"], m) for m in models])

    advanced_search.change(
        fn=update_model_list,
        inputs=advanced_search,
        outputs=model_dropdown
    )

    status_text = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", value="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞", interactive=False)

    with gr.Accordion("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏", open=False):
        # –ì—Ä—É–ø–ø–∞ –¥–ª—è Qwen (—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –≤—ã–±–æ—Ä–µ Whisper)
        with gr.Column(visible=True) as qwen_settings:
            gr.Markdown("### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Qwen")
            with gr.Row():
                dtype_choice = gr.Dropdown(choices=["float16", "bfloat16", "float32", "float8_e4m3fn", "float8_e5m2"],
                                           value="bfloat16", label="–¢–æ—á–Ω–æ—Å—Ç—å")
                quantization_choice = gr.Radio(choices=["None", "8-bit", "4-bit"], value="None", label="–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è")
            with gr.Row():
                use_flash = gr.Checkbox(value=True, label="Flash Attention 2")
                disable_talker = gr.Checkbox(value=True, label="–û—Ç–∫–ª—é—á–∏—Ç—å Talker")
            with gr.Row():
                device_map_strategy = gr.Dropdown(choices=["auto", "sequential", "balanced", "balanced_low_0", "custom"],
                                                 value="auto", label="device_map")
                custom_limits = gr.Textbox(label="–õ–∏–º–∏—Ç—ã –¥–ª—è custom", placeholder="20GB,20GB,15GB")
                offload_folder = gr.Textbox(label="Offload folder (CPU)", placeholder="e.g. ./offload")
            with gr.Row():
                max_audio_sec = gr.Slider(0, 600, value=0, step=10, label="–ú–∞–∫—Å. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ (—Å–µ–∫) (0 = –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏)")
                max_new_tokens = gr.Slider(128, 4096, 2048, step=64, label="–ú–∞–∫—Å. –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
            with gr.Row():
                do_sample = gr.Checkbox(value=False, label="do_sample")
                temperature = gr.Slider(0.0, 2.0, 0.1, step=0.1, label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞")
                repetition_penalty = gr.Slider(1.0, 2.0, value=1.0, step=0.05, label="Repetition penalty")
            gr.Markdown("### –ü—Ä–æ–º–ø—Ç—ã (—Ç–æ–ª—å–∫–æ –¥–ª—è Qwen)")
            with gr.Row():
                system_prompt_input = gr.Textbox(label="–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", value=DEFAULT_SYSTEM_PROMPT, lines=3, scale=4)
                reset_system_btn = gr.Button("‚Ü∫", scale=1, min_width=50, elem_classes="square-btn")
            with gr.Row():
                user_prompt_input = gr.Textbox(label="–ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –∑–∞–¥–∞—á–∏", value="", lines=2, scale=4)
                reset_user_btn = gr.Button("‚Ü∫", scale=1, min_width=50, elem_classes="square-btn")

        # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–≤–∏–¥–Ω—ã –≤—Å–µ–≥–¥–∞)
        gr.Markdown("### –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        with gr.Row():
            auto_save = gr.Checkbox(value=False, label="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        with gr.Row():
            output_dir = gr.Textbox(label="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø—É—Å—Ç–æ = —Ä—è–¥–æ–º —Å –∞—É–¥–∏–æ)", placeholder="", scale=4)
            browse_output_btn = gr.Button("üìÇ", scale=1, min_width=50, elem_classes="square-btn")
            browse_output_btn.click(
                fn=browse_folder,
                inputs=output_dir,
                outputs=output_dir
            )            
            console_progress = gr.Checkbox(value=False, label="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∫–æ–Ω—Å–æ–ª–∏")
        with gr.Row():
            noise_reduction = gr.Checkbox(value=False, label="–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ (noisereduce)")
        with gr.Row():
            use_segmentation = gr.Checkbox(value=False, label="–§—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ –∫—É—Å–∫–∏)")
            segment_duration = gr.Slider(10, 60, value=30, step=5, label="–î–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (—Å–µ–∫)")
        with gr.Row():
            use_stem_separation = gr.Checkbox(value=False, label="üé§ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ç–µ–º—ã (Demucs)")
            target_stem = gr.Dropdown(
                choices=["vocals", "instrumental", "drums", "bass", "other"],
                value="vocals",
                label="–¶–µ–ª–µ–≤–æ–π —Å—Ç–µ–º",
                interactive=False
            )
        with gr.Row():
            use_gpu_demucs = gr.Checkbox(value=False, label="üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è Demucs (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)")
        with gr.Row():
            debug_audio_checkbox = gr.Checkbox(value=False, label="üíæ –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª—å—é (debug)")

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ –≤—ã–ø–∞–¥–∞—é—â–µ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Ç–µ–º–æ–≤
        def toggle_stem_dropdown(use_stem):
            return gr.update(interactive=use_stem)
        use_stem_separation.change(
            fn=toggle_stem_dropdown,
            inputs=use_stem_separation,
            outputs=target_stem
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–∫—Ä—ã—Ç–∏–µ –±–ª–æ–∫–∞ Qwen –ø—Ä–∏ –≤—ã–±–æ—Ä–µ Whisper
        def toggle_qwen_settings(choice):
            return gr.update(visible=(choice == "Qwen Omni"))
        recognition_model.change(
            fn=toggle_qwen_settings,
            inputs=recognition_model,
            outputs=qwen_settings
        )

    # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–∞ –≤–≤–æ–¥–∞
    gr.Markdown("### –†–µ–∂–∏–º –≤–≤–æ–¥–∞")
    with gr.Row():
        input_mode = gr.Radio(choices=["–û–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª", "–ù–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤", "–ü–∞–ø–∫–∞"], value="–û–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª", label="")
    with gr.Column(visible=True) as single_col:
        audio_input = gr.Audio(type="filepath", label="–ê—É–¥–∏–æ—Ñ–∞–π–ª")
        text_input = gr.Textbox(label="–î–æ–ø. —Ç–µ–∫—Å—Ç", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: focus on vocals", lines=2)
    with gr.Column(visible=False) as multi_col:
        file_input = gr.Files(label="–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã", file_types=[".wav", ".mp3", ".flac", ".m4a", ".ogg"])
    with gr.Column(visible=False) as folder_col:
        with gr.Row():
            folder_input = gr.Textbox(label="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∞—É–¥–∏–æ", placeholder="C:/music/ –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ —á–µ—Ä–µ–∑ –∫–Ω–æ–ø–∫—É", scale=4)
            browse_folder_btn = gr.Button("üìÇ", scale=1, min_width=50, elem_classes="square-btn")
        browse_folder_btn.click(
            fn=browse_folder,
            inputs=folder_input,
            outputs=folder_input
        )

    def switch_mode(mode):
        return (
            gr.update(visible=(mode == "–û–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª")),
            gr.update(visible=(mode == "–ù–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤")),
            gr.update(visible=(mode == "–ü–∞–ø–∫–∞"))
        )
    input_mode.change(fn=switch_mode, inputs=input_mode, outputs=[single_col, multi_col, folder_col])

    with gr.Row():
        task_btn = gr.Button(value="‚¨áÔ∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å", variant="secondary", interactive=False, size="lg", scale=2)
        stop_btn = gr.Button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", variant="stop", scale=1)
        clear_cache_btn = gr.Button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à", variant="secondary", scale=1)

    output = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=15, interactive=False)

    with gr.Row():
        save_btn = gr.Button("üíæ –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é", variant="secondary")
        download_btn = gr.DownloadButton("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", visible=False, variant="primary")
        clear_all_btn = gr.Button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –ø–æ–ª—è –≤–≤–æ–¥–∞", size="lg")

    # –õ–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Qwen
    def load_qwen_and_update(model_info, dtype, quant, flash, talker, device_map, limits, offload):
        return load_qwen_model(model_info, dtype, quant, flash, talker, device_map, limits, offload)

    load_qwen_btn.click(
        fn=load_qwen_and_update,
        inputs=[model_dropdown, dtype_choice, quantization_choice, use_flash, disable_talker,
                device_map_strategy, custom_limits, offload_folder],
        outputs=[task_btn, status_text, user_prompt_input]
    )

    unload_qwen_btn.click(
        fn=unload_qwen_model,
        inputs=[],
        outputs=status_text
    ).then(
        fn=lambda: gr.update(value="‚¨áÔ∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å", variant="secondary", interactive=False),
        outputs=task_btn
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ Whisper
    def load_whisper_and_update(version):
        load_whisper_model(version)
        if whisper_pipeline is not None:
            return gr.update(value="üé§ –ó–∞–ø—É—Å—Ç–∏—Ç—å Whisper", variant="primary", interactive=True), "‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω"
        else:
            return gr.update(value="‚¨áÔ∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏", variant="secondary", interactive=False), "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Whisper"

    load_whisper_btn.click(
        fn=load_whisper_and_update,
        inputs=[whisper_version],  # –¥–æ–±–∞–≤–ª–µ–Ω–æ
        outputs=[task_btn, status_text]
    )

    def unload_whisper_and_update():
        unload_whisper_model()
        return gr.update(value="‚¨áÔ∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å", variant="secondary", interactive=False), "‚èπÔ∏è Whisper –≤—ã–≥—Ä—É–∂–µ–Ω"

    unload_whisper_btn.click(
        fn=unload_whisper_and_update,
        inputs=[],
        outputs=[task_btn, status_text]
    )

    reset_system_btn.click(fn=reset_system_prompt, outputs=system_prompt_input)
    reset_user_btn.click(fn=reset_user_prompt, outputs=user_prompt_input)

    clear_cache_btn.click(
        fn=lambda: ("üßπ –ö—ç—à –æ—á–∏—â–µ–Ω", torch.cuda.empty_cache() or gc.collect()),
        inputs=[],
        outputs=status_text
    )

    # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ task_btn)
    def run_task(mode, audio_file, text, file_list, folder, system, user,
                 max_sec, max_tokens, sample, temp, rep_penalty,
                 auto_save, out_dir, console_prog, noise_red, use_seg, seg_dur,
                 use_stem, target_stem, use_gpu, save_debug, rec_model, whisper_lang):
        global batch_stop_flag
        batch_stop_flag = False
        print(">>> run_task: batch_stop_flag —Å–±—Ä–æ—à–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        if rec_model == "Qwen Omni" and current_model is None:
            return "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Qwen."
        if rec_model == "Whisper Large v3" and whisper_pipeline is None:
            return "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å Whisper."

        try:
            if mode == "–û–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª":
                if not audio_file:
                    return "‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª."
                response = process_single_file(
                    audio_file, text, system, user,
                    max_sec, max_tokens, sample, temp, rep_penalty,
                    console_prog, noise_red, use_stem, target_stem, use_gpu, save_debug,
                    rec_model, use_seg, seg_dur, whisper_lang
                )
                if auto_save and not response.startswith("‚ö†") and not response.startswith("–û—à–∏–±–∫–∞") and not response.startswith("‚èπÔ∏è"):
                    model_type_for_save = current_model_type if rec_model == "Qwen Omni" else "whisper"
                    saved, msg = save_result(response, audio_file, out_dir, model_type_for_save)
                    if saved:
                        return response + f"\n\n{msg}"
                return response
            else:
                return batch_process(file_list if file_list else [],
                                     folder if mode == "–ü–∞–ø–∫–∞" else None,
                                     text, system, user,
                                     max_sec, max_tokens, sample, temp, rep_penalty,
                                     auto_save, out_dir, console_prog,
                                     noise_red, use_seg, seg_dur,
                                     use_stem, target_stem, use_gpu, save_debug,
                                     rec_model, whisper_lang)
        except Exception as e:
            return f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"

    task_event = task_btn.click(
        fn=run_task,
        inputs=[input_mode, audio_input, text_input, file_input, folder_input,
                system_prompt_input, user_prompt_input,
                max_audio_sec, max_new_tokens, do_sample, temperature, repetition_penalty,
                auto_save, output_dir, console_progress,
                noise_reduction, use_segmentation, segment_duration,
                use_stem_separation, target_stem, use_gpu_demucs, debug_audio_checkbox,
                recognition_model, whisper_language],
        outputs=output
    )

    stop_btn.click(
        fn=stop_generation,
        inputs=[],
        outputs=status_text
    )

    def handle_save(current_out, audio_path, out_dir, rec_model):
        if rec_model == "Qwen Omni" and current_model_type is None:
            return gr.update(), "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å"
        model_type_for_save = current_model_type if rec_model == "Qwen Omni" else "whisper"
        file_path, msg = prepare_download(current_out, audio_path, out_dir, model_type_for_save)
        if file_path:
            return gr.update(value=file_path, visible=True), msg
        else:
            return gr.update(visible=False), msg

    save_btn.click(
        fn=handle_save,
        inputs=[output, audio_input, output_dir, recognition_model],
        outputs=[download_btn, status_text]
    )

    def clear_all():
        return (
            None, "", DEFAULT_SYSTEM_PROMPT, reset_user_prompt(),
            0, 2048, False, 0.1, 1.0, "",
            None, None, None, None,
            gr.update(visible=False),
            False, False, 30,
            False, "vocals",
            False,
            False,
            False,
            "auto",                     # whisper_language
            "openai/whisper-large-v3"    # whisper_version
        )

    clear_all_btn.click(
        fn=clear_all,
        inputs=[],
        outputs=[audio_input, text_input, system_prompt_input, user_prompt_input,
                 max_audio_sec, max_new_tokens, do_sample, temperature, repetition_penalty, output_dir,
                 audio_input, file_input, folder_input, output,
                 download_btn,
                 noise_reduction, use_segmentation, segment_duration,
                 use_stem_separation, target_stem, use_gpu_demucs, debug_audio_checkbox,
                 recognition_model, whisper_language, whisper_version]  # –¥–æ–±–∞–≤–∏–ª–∏ whisper_version
    )

    gr.Markdown("---\n*–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏.*")

if __name__ == "__main__":
    demo.launch(inbrowser=True, css="""
    .square-btn {
        width: 40px !important;
        height: 40px !important;
        min-width: 40px !important;
        max-width: 40px !important;
        padding: 0 !important;
        font-size: 1.2em;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        flex: none !important;
    }
    """)